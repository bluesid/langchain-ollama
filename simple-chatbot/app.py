import streamlit as st
from langchain.chat_models import ChatOllama

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ğŸ¦™ ë­ë“ ì§€ ì§ˆë¬¸í•˜ì„¸ìš”~ ")
st.title('ğŸ¦™ ë­ë“ ì§€ ì§ˆë¬¸í•˜ì„¸ìš”~ ')

# ë‹µë³€ ìƒì„± í•¨ìˆ˜ (Ollama ì‚¬ìš©)
def generate_response(input_text):
    llm = ChatOllama(
        model="llama3",  # ì„¤ì¹˜ëœ Ollama ëª¨ë¸ëª… (ì˜ˆ: llama3, mistral, codellama ë“±)
        temperature=0,   # ì°½ì˜ì„± ì„¤ì •
    )
    response = llm.predict(input_text)
    st.info(response)

# ì§ˆë¬¸ ì…ë ¥ UI
with st.form('Question'):
    text = st.text_area('ì§ˆë¬¸ ì…ë ¥:', 'What types of text models does OpenAI provide?')
    submitted = st.form_submit_button('ë³´ë‚´ê¸°')
    if submitted:
        generate_response(text)