import streamlit as st
#from langchain_community.chat_models import ChatOllama
from langchain_ollama import ChatOllama

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ğŸ¦™ ë­ë“ ì§€ ì§ˆë¬¸í•˜ì„¸ìš”~ ")
st.title('ğŸ¦™ ë­ë“ ì§€ ì§ˆë¬¸í•˜ì„¸ìš”~ ')

# ë‹µë³€ ìƒì„± í•¨ìˆ˜ (Ollama ì‚¬ìš©)
def generate_response(input_text):
    llm = ChatOllama(
        model="EEVE-Korean-10.8B",  # ì„¤ì¹˜ëœ Ollama ëª¨ë¸ëª… (ì˜ˆ: llama3, mistral, codellama ë“±)
        temperature=0,   # ì°½ì˜ì„± ì„¤ì •
    )
    response = llm.invoke(input_text)
    # st.info(response)
    st.info(response.content)

# ì§ˆë¬¸ ì…ë ¥ UI
with st.form('Question'):
    text = st.text_area('ì§ˆë¬¸ ì…ë ¥:', 'What types of text models does OpenAI provide?')
    submitted = st.form_submit_button('ë³´ë‚´ê¸°')
    if submitted:
        generate_response(text)